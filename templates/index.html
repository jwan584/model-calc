<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üê≠Chinchilla2Llamaü¶ô</title>
    <link rel="stylesheet" href="/static/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@300;400;500;600;700;900&display=swap" rel="stylesheet">
    <meta name="description" content="Calculate how much data and compute you need to train a large language model using the Chinchilla or Llama method.">
    <meta name="keywords" content="Large Language Models, LLMs, Chinchilla, Llama, Cerebras-GPT">
</head>
<body>
    <div class="about-link">
        <a href="#" id="about-link">About</a>
    </div>
    <h1 id="h1_name">üê≠Chinchilla2Llamaü¶ô</h1>
    <h4>Estimate how much data and compute you need to train a llama style model</h4>
    <div class="container">
        <div class="left">
            <h2>Enter Your Model</h2>
            <label for="mode-selector">Mode:</label>
            <select id="mode-selector" name="mode-selector">
              <option value="chinchilla-to-llama" selected>Chinchilla => Llama</option>
              <option value="llama-to-chinchilla">Llama => Chinchilla</option>
            </select>
            <label for="parameters">Parameters (billions):</label>
            <input type="number" id="parameters">
            <label for="trainingTokens">Training Tokens (billions):</label>
            <input type="number" id="trainingTokens">
            <br>
            <label class="chinchilla-to-llama" for="chinchilla" style="display: inline-block;">Chinchilla Optimal:</label>
            <input  class="chinchilla-to-llama" type="checkbox" id="is_chinchilla" name="chinchilla" checked>
            <br><br>
            <h2 id="output-mode-h2">Make it a Llama Style Model</h2>
            <label class="chinchilla-to-llama" for="compactModelParameters">Compact Model Parameters (billions):</label>
            <input class="chinchilla-to-llama" type="number" id="compactModelParameters">
            <label for="inferences">Number of Inferences (billion tokens):</label>
            <input type="number" id="inferences" placeholder="optional">
            <button id="calculate">Calculate</button>
        </div>
        <div id="results"></div>
    </div>
    <div class="chart-container">
        <canvas id="myChart"></canvas>
    </div>
    <div id="modal" class="modal">
        <div class="modal-content">
            <span class="close">&times;</span>
            <h2>About</h2>
            <p>This calculator estimates the accuracy of large language models based on parameter size and data tokens. It uses the scaling laws published in the <a href="https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/">Cerebras-GPT paper</a>. The scaling law is applicable to LLM trained on the Pile dataset.</p>   
            <p>The tool allows you to choose between two distinct training styles - Chinchilla and Llama.</p>
            <p>The Chinchilla style uses a ratio of 20 tokens per parameter, producing a model with the least loss for a given computational capacity.</p>
            <p>On the other hand, the Llama style utilizes a significantly higher ratio of tokens per parameter. While this results in more compact models with improved accuracy, they demand more computational resources (FLOPs) during training. Although Llama models, being smaller, are more cost-efficient during inference, they require billions of inference tokens before the lower inference cost compensates for the higher initial training cost. This trade-off is shown in the "Inference Breakeven" figure.</p>
            <h2>Contact<h2>
            <p>James Wang <a href="https://twitter.com/draecomino" target="_blank"> @draecomino</a></p>
        </div>
    </div>
    <script src="/static/script.js"></script>
</body>
