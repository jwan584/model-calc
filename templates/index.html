<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>⚗️ Model Lab</title>
    <link rel="stylesheet" href="/static/styles.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script>var models = {{ models | tojson }};</script>
    <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@300;400;500;600;700;900&display=swap" rel="stylesheet">
    <meta name="description" content="Calculate how much data and compute you need to train a large language model using the Chinchilla or Llama method.">
    <meta name="keywords" content="Large Language Models, LLMs, Chinchilla, Llama, Cerebras-GPT">
</head>
<body>
    <div class="container">
        <div id="h1_div">
            <h1 id="h1_name">⚗️ Model Lab</h1>
            <div class="about-link"><a href="#" id="about-link">About</a></div>
            <h4>Compare different models, simulate runs, and estimate training & inference costs.</h4>
        </div>
        <div id="mode_div">
        <label for="mode" id="mode_label">Mode:</label>
            <select id="mode">
              <option value="compare" selected>Compare two models</option>
              <option value="iso_loss">Match Loss of Model 1</option>
            </select>
        </div>
        <div class="flex-container">
            <table id="input_table">
                <thead>
                  <tr>
                    <th></th>
                    <th>Model 1</th>
                    <th>Model 2</th>
                  </tr>
                </thead>
                <tbody>
                <tr>
                  <td>Select Model:</td>
                  <td>
                    <select id="model1_list">
                        {% for item in models %}
                             <option value="{{ item.name }}" {% if item.name == 'Cerebras-GPT 2.7B' %}selected{% endif %}>{{ item.name }}</option>
                        {% endfor %}
                    </select>
                  </td>
                  <td>
                    <select id="model2_list">
                        {% for item in models %}
                            <option value="{{ item.name }}" {% if item.name == 'Pythia 1B' %}selected{% endif %}>{{ item.name }}</option>
                        {% endfor %}
                    </select>
                  </td>
                </tr>
                  <tr>
                    <td>Parameters (B):</td>
                    <td><input type="number" id="params1"></td>
                    <td><input type="number" id="params2"></td>
                  </tr>
                  <tr>
                    <td>Tokens (B):</td>
                    <td><input type="number" id="tokens1"></td>
                    <td><input type="text" id="tokens2"></td>
                  </tr>
                  <tr>
                    <td>Inferences (B):</td>
                    <td><input type="number" placeholder="optional" id="inf1"></td>
                    <td><input type="number" placeholder="optional" id="inf2"></td>
                  </tr>
                </tbody>
              </table>
            </div>
            <button id="calculate">Calculate</button>
        </div>
    </div>
    <div id="results"></div>
        <div id="results_inner">
    <div id="total_compute_cost">
        <h2>Total Compute Cost</h2>
            <canvas id="myChart"></canvas>
    </div>
    <div id="loss_chart_div">
        <h2>Loss Curve</h2>
            <canvas id="lossChart"></canvas>
    </div>
    <div id="modal" class="modal">
        <div class="modal-content">
            <span class="close">&times;</span>
            <h2>About</h2>
            <p>This calculator estimates the accuracy of large language models based on the size of the model and the number of training tokens. It uses the scaling laws published in the <a href="https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/">Cerebras-GPT paper</a>. The scaling law is applicable to LLM trained on the Pile dataset.</p>   
            <p>The tool allows you to choose between two distinct training styles: Chinchilla and Llama.</p>
            <p>The Chinchilla style uses a ratio of 20 tokens per parameter, producing a model with the least loss for a given computational capacity.</p>
            <p>On the other hand, the Llama style utilizes a significantly higher ratio of tokens per parameter. While this results in more compact models with improved accuracy, they demand more computational resources during training. Although Llama models, being smaller, are more cost-efficient during inference, they require billions of inference tokens before the lower inference cost compensates for the higher initial training cost. This trade-off is shown in the "Inferences to Breakeven" figure. The "Users to Breakeven" figure assumes a single user generates 1k tokens per day for a year.</p>
            <h2>Contact<h2>
            <p>James Wang <a href="https://twitter.com/draecomino" target="_blank"> @draecomino</a></p>
        </div>
    </div>
    <script src="/static/script.js"></script>
</body>
